# sample config for metrictank
# the defaults here match the default behavior.

# tcp address for metrictank to bind to for its HTTP interface
listen = :6060

# accounting period to track per-org usage metrics
accounting-period = 5min

## clustering:
# cluster node name and value used to differentiate metrics between nodes
instance = default
# the primary node writes data to cassandra. There should only be 1 primary node per cluster of nodes
primary-node = false

## data:
# duration of chunks
chunkspan = 2h
# number of chunks to keep in memory. should be at least 1 more than what's needed to satisfy aggregation rules
numchunks = 5
# minimum wait before metrics are removed from storage
ttl = 35d

# max age for a chunk before to be considered stale and to be persisted to Cassandra
chunk-max-stale = 1h
# max age for a metric before to be considered stale and to be purged from memory
metric-max-stale = 6h
# Interval to run garbage collection job
gc-interval = 1h

# duration before secondary nodes start serving requests
warm-up-period = 1h

# settings for rollups (aggregation for archives)
# comma-separated of archive specifications.
# archive specification is of the form: aggSpan:chunkSpan:numChunks:TTL[:ready as bool. default true]
# with these aggregation rules: 5min:1h:2:3mon,1h:6h:2:1y:false
# 5 min of data, store in a chunk that lasts 1hour, keep 2 chunks in memory, keep for 3months in cassandra
# 1hr worth of data, in chunks of 6 hours, 2 chunks in mem, keep for 1 year, but this series is not ready yet for querying.
agg-settings =



### Metric data storage ###


## cassandra
# comma-separated list of hostnames to connect to
cassandra-addrs = 
# desired write consistency (any|one|two|three|quorum|all|local_quorum|each_quorum|local_one
cassandra-consistency = one
# cassandra timeout in milliseconds
cassandra-timeout = 1000
# max number of concurrent reads to cassandra
cassandra-read-concurrency = 20
# max number of concurrent writes to cassandra
cassandra-write-concurrency = 10
# max number of outstanding reads before blocking. value doesn't matter much
cassandra-read-queue-size = 100
# write queue size per cassandra worker. should be large engough to hold all at least the total number of series expected, divided by how many workers you have
cassandra-write-queue-size = 100000
# CQL protocol version. cassandra 3.x needs v3 or 4.
cql-protocol-version = 4


### Metric metadata index ###


## elasticsearch
# elasticsearch address for metric definitions
elastic-addr = localhost:9200
# index name for storing metric index
index-name = metric


### Profiling, instrumentation and logging ###

# see https://golang.org/pkg/runtime/#SetBlockProfileRate
block-profile-rate = 0
# 0 to disable. 1 for max precision (expensive!) see https://golang.org/pkg/runtime/#pkg-variables")
mem-profile-rate = 524288 # 512*1024

# statsd address
statsd-addr = localhost:8125
# standard or datadog
statsd-type = standard

# inspect status frequency. set to 0 to disable
proftrigger-freq = 60s
# path to store triggered profiles
proftrigger-path = /tmp
# minimum time between triggered profiles
proftrigger-min-diff = 1h
# if this many bytes allocated, trigger a heap profile
proftrigger-heap-thresh = 10000000

# only log incoming requests if their timerange is at least this duration. Use 0 to disable
log-min-dur = 5min

# only log log-level and higher. 0=TRACE|1=DEBUG|2=INFO|3=WARN|4=ERROR|5=CRITICAL|6=FATAL
log-level = 2


### inputs section ###

# NSQ input (discouraged)
[nsq-in]
enabled = false
# topic for metrics
topic = metrics
# channel for metrics. leave empty to generate random ephemeral one
channel = tank
# comma separated list of tcp addresses of nsqd servers
nsqd-tcp-address = 
# comma separated list of http addresses of lookupd servers
lookupd-http-address = 
# max number of messages to allow in flight
max-in-flight = 200
# number of workers parsing messages from NSQ
concurrency = 10
# passthrough to nsq.Producer (may be given multiple times as comma-separated list, see http://godoc.org/github.com/nsqio/go-nsq#Config)")
producer-opt = 
#passthrough to nsq.Consumer (may be given multiple times as comma-separated list, http://godoc.org/github.com/nsqio/go-nsq#Config)")
consumer-opt = 

## carbon input (optional)
[carbon-in]
enabled = false
# tcp address
addr = :2003
# needed to know your raw resolution for your metrics. see http://graphite.readthedocs.io/en/latest/config-carbon.html#storage-schemas-conf
schemas-file = /path/to/your/schemas-file

## kafka-all input (optional, recommended)
## it's like mdm but also cluster msgs in same topic
[kafka-all]
enabled = false
# tcp address (may be given multiple times as a comma-separated list)
brokers = kafka:9092
# kafka topic (may be given multiple times as a comma-separated list)
topics = mdm
# consumer group name
group = group1
# The minimum number of message bytes to fetch in a request
consumer-fetch-min = 1024000
# The default number of message bytes to fetch in a request
consumer-fetch-default = 4096000
# The maximum amount of time the broker will wait for Consumer.Fetch.Min bytes to become available before it
consumer-max-wait-time = 1s
#The maximum amount of time the consumer expects a message takes to process
consumer-max-processing-time = 1s
# How many outstanding requests a connection is allowed to have before sending on it blocks
consumer-fetch-default = 100

## kafka-mdm input (optional)
[kafka-mdm-in]
enabled = false
# tcp address (may be given multiple times as a comma-separated list)
brokers = kafka:9092
# kafka topic (may be given multiple times as a comma-separated list)
topics = mdm
# consumer group name
group = group1
# The minimum number of message bytes to fetch in a request
consumer-fetch-min = 1024000
# The default number of message bytes to fetch in a request
consumer-fetch-default = 4096000
# The maximum amount of time the broker will wait for Consumer.Fetch.Min bytes to become available before it
consumer-max-wait-time = 1s
#The maximum amount of time the consumer expects a message takes to process
consumer-max-processing-time = 1s
# How many outstanding requests a connection is allowed to have before sending on it blocks
consumer-fetch-default = 100

## kafka-mdam input (optional, discouraged)
[kafka-mdam-in]
enabled = false
# tcp address (may be given multiple times as a comma-separated list)
brokers = kafka:9092
# kafka topic (may be given multiple times as a comma-separated list)
topics = mdam
# consumer group name
group = group1


### clustering transports ###
# kafka as transport for clustering messages (recommended)
[kafka-cluster]
enabled = false
# tcp address (may be given multiple times as a comma-separated list)
broker = kafka:9092
# kafka topic (only one)
topic = metricpersist
# consumer group name
group = group1

# nsq as transport for clustering messages
[nsq-cluster]
enabled = false
# nsqd TCP address (may be given multiple times as comma-separated list)
nsqd-tcp-address = 
# lookupd HTTP address (may be given multiple times as comma-separated list)
lookupd-http-address = 
topic = metricpersist
channel = tank
# passthrough to nsq.Producer (may be given multiple times as comma-separated list, see http://godoc.org/github.com/nsqio/go-nsq#Config)")
producer-opt = 
#passthrough to nsq.Consumer (may be given multiple times as comma-separated list, http://godoc.org/github.com/nsqio/go-nsq#Config)")
consumer-opt = 
# max number of messages to allow in flight
max-in-flight = 200

