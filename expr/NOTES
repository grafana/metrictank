## future code quality improvements

The code is a bit ugly in some places, there's a few items I'm not happy with and hope to change down the road, ideally after building out the feature more and getting a better understanding of requirements. In particular:

* do all validation in planner, nothing should be needed in individual functions Init invocations
  though often validation and storing a value are intertwined. should ideally feed function post-validation value (e.g. movingAverage windowsize)
* in func Exec, uniform access to optional args, whether specified via kwarg or position.
* Init and NeedRange could be 1 function, though not sure if they should
* we instantiate and init functions twice (during creation of plan and running it). could do this only once if ...
function invocations could be attached to expressions (when creating the plan). when executing them, we then
also don't need to pass args anymore (which is currently redundant since they already got them via Init)
* in general, while constructing the plan, we should probably build a different datastructure more tailored towards execution, currently we
  stick to the parse-time expr types in the planner and these concepts also make their way into the function implementations, which gets a bit dirty at times.

movingAverage accepts arg as both int or string, how to handle properly?


## regarding when to allocate models.Series (or more precisely, their []schema.Point attribute)

there's 2 main choices:

1) copy-on-write:
- each function does not modify data in their inputs, they allocate new slices (or better: get from pool) etc to save output computations
- storing data into new slice can be done in same pass as processing the data
- potentially many of these steps in a sequence if you have lots of processing steps in a row
- getting a slice from the pool may cause a stall if it's not large enough and runtime needs to re-allocate and copy
- allows us to pass the same data into multiple processing steps

2)copy in advance:
- give each processing step a copied slice in which they can do whatever they want (e.g. modify in place)
- works well if you have many processing steps in a row that can just modify in place
- copying up front, in a separate pass. also causes a stall
- often copying may be unnessary, but we can't know that in advance (unless we expand the expr tree to mark whether it'll do a write)


I'm going to assume that multi-steps in a row is not that common, and COW seems more commonly the best approach


This leaves the problem of effectively managing allocations and using a sync.Pool.
Note that the expr library can be called by different clients (MT, grafana, graphite-ng, ...)
It's up to the client to instantiate the pool, and set up the default allocation to return point slices of desired point capacity.
The client can then of course use this pool to store series, which it then feeds to expr.
expr library does the rest.  It manages the series/pointslices and gets new ones as a basis for the COW.
Once the data is returned to the client, and the client is done using the returned data, it should call plan.Clean(),
which returns all data back to the pool  (both input data or newly generated series, whether they made it into the final output or not).

note that individual processing functions only request slices from the pool, they don't put anything on it.
e.g. an avg of 3 series will create 1 new series (from pool), but won't put the 3 inputs back in the pool, because
another processing step may require the same input data.

function implementations:
* must not modify existing slices
* should use the pool to get new slices in which to store their new/modified data. and add the new slice into the cache so it can later be cleaned
